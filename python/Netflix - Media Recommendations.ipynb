{"cells":[{"cell_type":"markdown","source":["# References\n## Data Sets\n* Netflix data set: https://opendata.stackexchange.com/questions/7883/netflix-data-set/7884\n  * https://www.kaggle.com/netflix-inc/netflix-prize-data\n  * https://archive.org/details/nf_prize_dataset.tar\n    * https://archive.org/download/nf_prize_dataset.tar/nf_prize_dataset.tar.gz (665 MB)\n  * https://web.archive.org/web/20090925184737/http://archive.ics.uci.edu/ml/datasets/Netflix+Prize\n  * http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a\n* MovieLens: https://grouplens.org/datasets/movielens/\n  * http://files.grouplens.org/datasets/movielens/ml-20m.zip (190 MB)\n## Source Code\n* Source code: http://github.com/telecoms-intelligence/netflix-study\n# Dependencies\n* That notebook needs a lot of memory to read the Netflix movie rating files, at least 64GB.\nA cluster known to run properly is ``Single - 122GB - DB 33.3``"],"metadata":{}},{"cell_type":"code","source":["%sh\n# It takes roughly 3 minutes on 'Single - 122 GB'\n# pip install -U pip\npip install -U word2veckeras"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import os, re, logging\nimport random\nimport datetime as dt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport gensim.models as gm\nimport matplotlib.pyplot as plt\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sh\n\nls -laFh /dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%fs\nls /mnt/data-science/use-cases/uc01-recommendation-engine/netflix"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sh\n\nhead -5 /dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/training_set/mv_0000190.txt"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Jump to [Read the DataFrame from a Pickle file section to avoid re-reading the Netflix data files](#notebook/5005/command/5698)"],"metadata":{}},{"cell_type":"code","source":["# Takes 27-30 hours on the 'Single - 122 GB - DB 3.3' cluster\ndef read_nf_training_set(nf_filepath):\n  file_col_names = ['User', 'Rating', 'DoR']\n  file_col_types = {'User': np.int64, 'Rating': np.int64}\n  \n  files = os.listdir(nf_filepath)\n  # Read all files from the directory\n  df = pd.DataFrame()\n  for fil_index, fil in enumerate(files):\n    if fil_index % 100 == 0:\n        print (fil_index)\n    df_tmp = pd.read_csv(nf_filepath + fil, names = file_col_names, skiprows=[0], parse_dates = [2], dtype = file_col_types)\n    name = re.findall(r'\\d+', fil)\n    df_tmp = df_tmp.reset_index(drop=True)\n    df_tmp['Mov'] = name * len(df_tmp)\n    df = pd.concat([df, df_tmp], axis=0)\n    \n  print('done')\n  return df\n\n#\n#df = read_nf_training_set(\"/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/training_set/\")\n#df.dtypes"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Dump/Serialize the DataFrame into a file (with Pickle)\n# Note that the Pickle format/protocol is highly dependent on the Python version used. Here Python 2 is used,\n# and later Pickle versions (eg, protocol version 4) are not supported\n# Takes almost 13 minutes on the 'Single - 122 GB - DB 3.3' cluster\n#df.to_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nfc.pkl')\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Takes almost 2 minutes on the 'Single - 122 GB - DB 3.3' cluster\ndf = pd.read_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nfc.pkl')"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# The following has to be uncommented when 'df' is read from 'nfb.pkl', rather than from 'nfc.pkl'\n# Takes 30 seconds on the 'Single - 122 GB - DB 3.3' cluster\n#df['DoV'] = df['DoV'].astype('datetime64[ns]')\ndf.dtypes"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Temporary hack for 'nfb.pkl', which misses movie rating data files\ndef read_nf_missing_training_set_from_file(df_glob, nf_name, nf_filepath):\n  file_col_names = ['User', 'Rating', 'DoV']\n  file_col_types = {'User': np.int64, 'Rating': np.int64}\n  \n  df_tmp = pd.read_csv(nf_filepath, names = file_col_names, skiprows = [0], parse_dates = [2], dtype = file_col_types)\n  df_tmp = df_tmp.reset_index()\n  del df_tmp['index']\n  df_tmp['Mov'] = nf_name * len(df_tmp)\n  df_glob = pd.concat([df_glob, df_tmp], axis=0)\n  return df_glob\n\n#\ndef read_nf_missing_training_set():\n  df_glob = pd.DataFrame()\n  with open('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/missing_files.txt') as lfile:\n    missing_files = lfile.readlines()\n  missing_files = [x.strip() for x in missing_files]\n\n  #\n  for missing_file in missing_files:\n    missing_filepath = \"/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/training_set/\" + missing_file\n    missing_movie_name_as_list = re.findall(r'\\d+', missing_file)\n    movie_name = missing_movie_name_as_list[0]\n    print(\"Name: \" + movie_name + \", filepath: \" + missing_filepath)\n    df_glob = read_nf_missing_training_set_from_file(df_glob, movie_name, missing_filepath)\n\n  return df_glob\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Takes 2 minutes on the 'Single - 122 GB - DB 3.3' cluster\n#df_missing = read_nf_missing_training_set()\n#df_missing.dtypes"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#df = pd.concat([df, df_missing], axis=0)\n#df.dtypes"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# The following has to be uncommented when 'df' is read from 'nfb.pkl', rather than from 'nfc.pkl'\n#df['DoR'] = df['DoV']\n#df = df.drop(['DoV'], axis = 1)\n#df.dtypes"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Aggregate the movie reviews per (movie, date of review) pairs.\n# For each movie, the number of unique users and the average rating are computed.\n# Takes 4 minutes on the 'Single - 122 GB - DB 3.3' cluster\nh = {'User': ['count'], 'Rating': ['mean']}\ndf_mov_g = df.groupby(['Mov', 'DoR']).agg(h).reset_index()\ndf_mov_g.columns = ['Mov', 'DoR', 'Rating_mean', 'User_count']\n\n# Sort by date of review\ndf_mov_g = df_mov_g.sort_values(['Mov', 'DoR'])\n\n# Add a column materializing the change of movie\ndf_mov_g['first_day_mov'] = np.where(df_mov_g.Mov != df_mov_g.Mov.shift(1), 1, 0)\n\n# Keep only the first date of review for every movie\ndf_mov_g = df_mov_g[df_mov_g['first_day_mov'] == 1]\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Please see [comment about dumping DataFrame with Pickle](#notebook/5005/command/5703)"],"metadata":{}},{"cell_type":"code","source":["# Takes almost 13 minutes on the 'Single - 122 GB - DB 3.3' cluster\n#df_mov_g.to_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nfb_mov_first.pkl')"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#df_mov_g = pd.read_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nfb_mov_first.pkl')\n#df_mov_g.dtypes\ndf_mov_g"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["fig, ax = plt.subplots()\n\nax.set_ylim((0,5))\nax.set_xlim((0,700))\nax.set_xlabel('Number of times movie watched by unq viewrs on the release day', fontsize=18)\nax.set_ylabel('Average rating', fontsize=18)\nax.scatter(df_mov_g.User_count, df_mov_g.Rating_mean) \n# Conclusion- To solve cold start problem for a movie, there is no pattern\n# New Movies should be indexed by some meta data\n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Takes 50 seconds on the 'Single - 122 GB - DB 3.3' cluster\nf = {'Mov': ['count'], 'Rating': ['mean']}\ndf_usr_g = df.groupby(['User','DoR']).agg(f).reset_index()\ndf_usr_g.columns = ['User', 'DoR', 'Rating_mean', 'Mov_count']\n\ndf_usr_g= df_usr_g.sort_values(['User', 'DoR'])\ndf_usr_g['first_day_usr'] = np.where(df_usr_g.User != df_usr_g.User.shift(1), 1, 0 )\ndf_usr_g= df_usr_g[df_usr_g['first_day_usr'] == 1]\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Please see [comment about dumping DataFrame with Pickle](#notebook/5005/command/5703)"],"metadata":{}},{"cell_type":"code","source":["# Takes almost 13 minutes on the 'Single - 122 GB - DB 3.3' cluster\n#df_usr_g.to_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nfb_usr_first.pkl')"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#df_usr_g = pd.read_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nfb_usr_first.pkl')\n#df_usr_g.dtypes\ndf_usr_g"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["fig, ax = plt.subplots()\n\nax.set_ylim((0,5))\nax.set_xlim((0,2000))\nax.set_xlabel('Number of movies rated by unq user on their first day', fontsize=18)\nax.set_ylabel('Average rating', fontsize=18)\nax.scatter(df_usr_g.Mov_count, df_usr_g.Rating_mean) \n# Conclusion- To solve cold start problem for a user, there is no pattern\n# New Users should be asked in some way\n# However when users rate a lot of movie it reverts to mean\n\ndisplay(fig)\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["df.dtypes"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["df_usr_g.dtypes"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Takes 25 minutes on the 'Single - 122 GB - DB 3.3' cluster\ndf_join = pd.merge(df, df_usr_g, on='User', how='left', suffixes=('_x', '_y'))\ndf_join = df_join.drop(['Rating_mean', 'Mov_count', 'first_day_usr'], axis=1)\ndf_join['day_count'] = (df_join['DoR_x'] - df_join['DoR_y']).dt.days\ndf_join = df_join.drop(['DoR_y'], axis=1)\ndf_join.columns = ['User', 'Rating', 'DoR', 'Mov', 'day_count']\ndf_join.dtypes"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Please see [comment about dumping DataFrame with Pickle](#notebook/5005/command/5703)"],"metadata":{}},{"cell_type":"code","source":["# Takes almost 13 minutes on the 'Single - 122 GB - DB 3.3' cluster\n#df_join.to_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nfb_join.pkl')"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["#df_join = pd.read_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nfb_join.pkl')\ndf_join"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Takes 45 seconds on the 'Single - 122 GB - DB 3.3' cluster\ng = {'Rating': ['mean', 'count' , 'std']}\ndf_time = df_join.groupby(['User', 'day_count']).agg(g).reset_index()\ndf_time.columns = ['User', 'day_count', 'rating_mean', 'rating_count', 'rating_std']"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["df_time"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["df_time_t = df_time[df_time['User'] == 6]"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["df_time_t.sort_values(['day_count'])"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["fig, ax = plt.subplots()\n\nax.set_ylim((0,5))\nax.set_xlim((0,600))\nax.set_xlabel('Customer Days since his first rating', fontsize=18)\nax.set_ylabel('Average rating', fontsize=18)\nax.scatter(df_time_t.day_count, df_time_t.rating_mean) \n#Users have a global baseline. They only like to rate above a baseline\n#Hence models have to be designed to give special preference to low weights as heavily biased \n# towards positive ones\n# some people have chosen to rank all the movies in a day 2040859 \n# These users habitually rank all the movie\n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["* Is there a change in taste of customer\n* Is there a change in rating behaviour of customer in long term"],"metadata":{}},{"cell_type":"code","source":["# Just check the 'Mov' column (movie ID)\ndf_join['Mov']"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# Takes 15 minutes on the 'Single - 122 GB - DB 3.3' cluster\n# mov_ind goes up to 100,140,000 (100 millions), and takes roughly 15 minutes on 'Single - 122 GB'\nmovie = df_join['Mov'].tolist()\nday_count = df_join['day_count'].tolist()\nsentence = []\nsentence_tmp = []\nsentence_tmp.append(str(movie[0]))\nfor mov_ind, mov in enumerate(movie):\n    if mov_ind % 10000 == 0:\n        print(mov_ind)\n    if mov_ind > 0:\n        if day_count[mov_ind] !=  day_count[mov_ind-1]:\n            sentence.append(sentence_tmp)\n            sentence_tmp = []\n        sentence_tmp.append(str(mov))\n\nsentence"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# sentence is a list, and cannot be dumped with Pickle.\n# Shelve (https://docs.python.org/3/library/shelve.html) may be used instead\n#sentence.to_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nf_sentence.pkl')"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# See above about Pickle vs Shelve\n#sentence = pd.read_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nf_sentence.pkl')"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# Step 3. Vector representation of movies\n# It takes roughly 13 minutes on 'Single - 122 GB'\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 100    # Word vector dimensionality                      \nmin_word_count = 3   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 1         # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\nhs =1\nnegative =0\n# Initialize and train the model (this will take some time)\nprint(\"Training model...\")\n# gm stands for gensim.models\nmodel = gm.word2vec.Word2Vec(sentence, workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\nmodel.init_sims(replace=True)\n\nmodel_name = \"100features_3minmovs_1context\"\nmodel.save(model_name)\n"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%sh\n\ngrep -e \"15124\" -e \"15164\" -e \"15246\" -e \"15436\" /dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/movie_titles.txt\n"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# 15124, 1996, Independence Day\n# 15164, 2000, The X-Files: Season 8\n# 15246, 1992, Alien 3: Collector's Edition\n# 15436, 1998, A Perfect Murder\nmodel.doesnt_match(['0015124', '00015164', '0015246', '0015436'])\n"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# 15124,1996,Independence Day\n# 15164,2000,The X-Files: Season 8\n# 15246,1992,Alien 3: Collector's Edition\n# 15436,1998,A Perfect Murder\nmodel.wv.most_similar(positive=['0015164', '0015246'], negative=['0015436'])\n#model.score(['0015124','00015164','0015246','0015436'])\n"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%sh\n\nhead -5 /dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/training_set/mv_0009733.txt"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# Takes 60 minutes on the 'Single - 122 GB - DB 3.3' cluster\n# For a while, the mv_0009733.txt file (and many others) was missing, triggering the following error:\n# KeyError: \"word '0009733' not in vocabulary\"\ntr = []\nfor item in range(1, 17771):\n    item_r = str('000000') + str(item)\n    item_r = item_r[-7:]\n    tr.append(model.wv[item_r])\n"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["tr = np.array(tr)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["# Takes 4 minutes on 'Single - 122 GB'\n# TSNE comes from sklearn.manifold\nmodel_tsne = TSNE(n_components=2)\nvis_tsne = model_tsne.fit_transform(tr)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["fig, ax = plt.subplots()\n\nvis_x = vis_tsne[:, 0]\nvis_y = vis_tsne[:, 1]\n\nax.scatter(vis_x, vis_y,s=1)\n\ndisplay(fig)\n"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# Takes 16 minutes on 'Single - 122 GB - DB 3.3'\n# TSNE comes from from sklearn.manifold\nmodel_tsne = TSNE(n_components=3)\nvis_tsne3 = model_tsne.fit_transform(tr)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# Takes 2 minutes on the 'Single - 122 GB - DB 3.3' cluster\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import *\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\nfor item in range(0, len(vis_tsne)):\n    ax.scatter(vis_tsne3[item,0], vis_tsne3[item,1], vis_tsne3[item,2],s=1)\n    #ax.text(vec[item,0], vec[item,1], vec[item,2], '%s' % (str(text[item])), size = 7,\n                #zorder =1, color ='r')\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n\ndisplay(fig)\n"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# Takes 1 minute on the 'Single - 122 GB - DB 3.3' cluster\ndf_h = df.copy(deep = False)\ndf_h.sort_values('User', inplace = True)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# Takes 15 minutes on the 'Single - 122 GB - DB 3.3' cluster\n#df_h.to_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nf_intermediary.pkl')"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["#df_h = pd.read_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nf_intermediary.pkl')"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# Takes 2 minutes on the 'Single - 122 GB - DB 3.3' cluster\nmodel = gm.Word2Vec.load(\"100features_3minmovs_1context\")\nprint('done')"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["# Takes 11 minutes of the 'Single - 122 GB - DB 3.3' cluster\nuser = df_h['User'].tolist()\nrating = df_h['Rating'].tolist()\nmov = df_h['Mov'].tolist()\ndor =  df_h['DoR'].tolist()"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["# Takes 1.2 hour of the 'Single - 122 GB - DB 3.3' cluster\nflag = []\npast_m = []\npast_s = []\ncurrent_m = []\ncurrent_s = []\nu = []\nuser_dic = {}\nuser_count = 0\nag = []\nnb_user = len(user)\nuser_not_fit = []\nuser_not_fit_dict = {}\nfor counter in range(0, nb_user-5):\n  # Users where we do not have the 5 movies in history will not be recommended from this model\n  current_user = user[counter]\n  if current_user == user[counter+5]:\n    # setting the flag\n    if (dor[counter+5] - dor[counter]).days > 30:\n      flag.append(1)\n    else:\n      flag.append(0)\n    # setting the user\n    if not current_user in user_dic:\n      user_count += 1\n      user_dic[current_user] = user_count\n\n    past_m_tmp = []\n    past_s_tmp = []\n        \n    for c in range(0, 5):\n      past_m_tmp.append(model.wv[mov[counter+c]])\n      past_s_tmp.append(rating[counter+c])\n    past_m.append(past_m_tmp)\n    past_s.append(past_s_tmp)\n    ag.append(sum(past_s_tmp)/5)\n    current_m.append(model.wv[mov[counter+5]])\n    current_s.append(rating[counter+5])\n    u.append(user_dic[current_user])\n  else:\n    user_not_fit.append(current_user)\n    if not current_user in user_not_fit_dict:\n      user_not_fit_dict[current_user] = 1\n    pass\n    \n  interval = int(nb_user / 100)\n  if counter % interval == 0:\n    print (\"Done \" + str(int(100 * counter / nb_user)) + \"%\")\nprint('done')\n"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["print(\"Nb of rows: \" + str(nb_user/1e6) + \", nb of fitting rows: \" + str(len(user_dic)/1e6) + \", nb of non fitting rows: \" + str(len(user_not_fit)/1e6))\nprint(\"user_dic: \" + str(user_dic))\nprint(\"user_not_fit_dict: \" + str(user_not_fit_dict))"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["df_train = pd.DataFrame()\ndf_train['user'] = u\ndf_train['past_m']= past_m\ndf_train['past_s']= past_s\ndf_train['current_m'] = current_m\ndf_train['current_s'] = current_s\ndf_train['ag'] = ag\ndf_train['flag'] = flag"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["# Takes 15 minutes on the 'Single - 122 GB - DB 3.3' cluster\n#df_train.to_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nf_train_mod.pkl')"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["#df_train = pd.read_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nf_train_mod.pkl')\ndf_train"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["# df_train = pd.read_pickle('/dbfs/mnt/data-science/use-cases/uc01-recommendation-engine/netflix/nf_train_mod.pkl')\ndf_train = df_train.sample(frac=1)\ndf_train_r = df_train[((df_train['flag']== 0) & (df_train['ag'] < 2.5)) | ((df_train['flag']== 0) & (df_train['ag'] > 3.5))]\n"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["X1= np.array(df_train_r['past_m'].tolist())\nX1=X1.reshape(len(X1),5, 100,1)\nX2= np.array(df_train_r['past_s'].tolist())\nX2=X2.reshape(len(X2),5)\nX4= np.array(df_train_r['current_m'].tolist())\nX4=X4.reshape(len(X4),1,100)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["from keras.layers import Embedding\nfrom keras.layers import Input, Convolution2D, Dense, merge, Flatten, Dropout, MaxPooling2D, Input, Reshape\nfrom keras.models import Model\nX3= np.random.random((250,100))\nMAX_SEQUENCE_LENGTH = 1\nEMBEDDING_DIM =100\nembedding_layer = Embedding(250,\n                            EMBEDDING_DIM,\n                            weights=[X3],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True)\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\nembedded_sequences = embedding_layer(sequence_input)\nem = np.array(df_train_r['user'].tolist())"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["y= np.array(df_train_r['current_s'].tolist())\ny= y.reshape(len(y),1)\ny_class = []\nfor g in range(0, len(y)):\n    tmp = [0] * 5\n    tmp[y[g]-1] = 1\n    y_class.append(tmp)\ny_class = np.array(y_class).reshape(len(y_class),1,5)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["#building the model in Keras with 2 models sharing the weights\n\nfrom keras.layers import Input, Convolution2D, Dense, merge, Flatten, Dropout, MaxPooling2D, Input, Reshape\nfrom keras.models import Model\n\npool_size = (2, 1)\nmax_val=5\nnb_epoch=10\nbatch_size=128\n# convolution kernel size\nnb_filters = 1200\nkernel_size1 = (1, 100)\n\nprint('start...')\n\n#Input1\nimg1 = Input(shape=(max_val, 100,1))#Just the shape matters hence addition of 1 \n\n#Convolution\n\nimg1_layer1 = Convolution2D(nb_filters, kernel_size1[0], kernel_size1[1],\n                        border_mode='valid',activation='relu')(img1)\n\n#Aggregating the filter output by flattening and comparing the value\nimg1_layer2= Flatten()(img1_layer1)\nimg1_layer3 = Dense(5)(img1_layer2)\n#Input2\n\nimg2 = Input(shape=(5,))#Just the shape matters hence addition of 1 \n\n# Merging them\n\nmerge_layer1 = merge([img1_layer3, img2], mode = 'sum')\nmerge_layer1_expand = Dense(100)(merge_layer1 )\nreshape = Reshape((1,100))(merge_layer1_expand)\n# Bringing embedding layer ,  current movie and the flag\nimg3 = Input(shape=(1,100))\nmerge_layer2 = merge([embedded_sequences, img3,], mode = 'sum')\nmerge_layer3 = merge([merge_layer2 , reshape], mode = 'sum')\n#img1_layer4 = Dense(5, activation ='relu')(merge_layer2)\n\npredictions = Dense(5, activation='softmax')(merge_layer3)\n\n\nmodel = Model(input=[img1, img2, sequence_input,img3 ], output=[predictions] )\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer= 'adagrad',\n              metrics=['accuracy'])\n\nmodel.fit([X1[:640000],X2[:640000], em[:640000],X4[:640000]], y_class[:640000], batch_size=batch_size, nb_epoch=nb_epoch,\n           validation_data=([X1[640000:],X2[640000:], em[640000:],X4[640000:]],[y_class[640000:]]),verbose=1)\nprint('done')"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["# Prediction on 4000 Recommendations\npredicted = model.predict([X1[640000:],X2[640000:], em[640000:],X4[640000:]], verbose=0) \npr1 = []\npr2 =[]\nfor rows in range(0,len(predicted)):\n    score= np.argsort(predicted[rows])[-2:][0]\n    pr1.append(score[-1]+1)\n    pr2.append(score[-2]+1)\ndf_visual = pd.DataFrame()\ndf_visual['User'] = em[640000:]\ndf_visual['Actual'] = y[640000:,0]\ndf_visual['Rating1_score'] = predicted[:,0,0]\ndf_visual['Rating2_score'] = predicted[:,0,1]\ndf_visual['Rating3_score'] = predicted[:,0,2]\ndf_visual['Rating4_score'] = predicted[:,0,3]\ndf_visual['Rating5_score'] = predicted[:,0,4]\ndf_visual['First_Predicted_Rating'] =  pr1\ndf_visual['Second_Predicted_Rating'] =  pr2\ndf_visual_good = df_visual[(df_visual['Actual'] == df_visual['First_Predicted_Rating'] ) | (df_visual['Actual'] == df_visual['Second_Predicted_Rating'] )]\ndf_visual_bad = df_visual[(df_visual['Actual'] != df_visual['First_Predicted_Rating'] ) & (df_visual['Actual'] != df_visual['Second_Predicted_Rating'] )]\n\nprint('done')"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["df_visual"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["df_visual_good"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["df_visual_bad"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":72}],"metadata":{"name":"UC 01 - Media Recommendations","notebookId":5005},"nbformat":4,"nbformat_minor":0}
